---
title: "p8105_hw5_yy3563"
author: "Yifei Yu"
date: "2024-11-04"
output: github_document
---

Load some packages.
```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
```

## Problem 1

```{r}
bday_sim = function(n) {
  
  bdays = sample(1:365, size = n, replace = TRUE)
  
  duplicate = length(unique(bdays)) < n
  
  return(duplicate)
}
```

```{r}
sim_res =
  expand_grid(
    n = 2:50,
    iter = 1:10000
  ) |> 
  mutate(res = map_lgl(n, bday_sim)) |> 
  group_by(n) |> 
  summarize(prob = mean(res))

sim_res |> 
  ggplot(aes(x = n, y = prob)) +
  geom_line()
```

The probability of a shared birthday rises quickly as the group size increases. As the group size approaches 50, the probability becomes close to 1, meaning that it becomes almost certain for larger groups to have at least one shared birthday.


## Problem 2

```{r}
library(broom)

n = 30
sigma = 5

sim_power = function(mu){
  
  tibble(
    x = rnorm(n, mu, sigma)) |> 
      summarize(
        tidy(t.test(x, mu = 0, conf.level=0.95))) |> 
    select(estimate, p.value)
  
}

sim_result = expand_grid(
  mu = 0:6,
  iter = 1:5000) |> 
  mutate(samp_res = map(mu, sim_power)) |> 
  unnest(samp_res)
```

```{r}
sim_result |> 
  group_by(mu) |> 
  summarize(
    power = mean(p.value < 0.05)
  ) |> 
  ggplot(aes(x = mu, y = power)) +
  geom_point() +
  geom_line()
```

As the effect size (the value of mu) increases, the power of the test also increases. In other words, the probability of rejecting the null hypothesis  increases with larger deviations of mu from zero.

```{r}
df1 = sim_result |> 
  group_by(mu) |> 
  summarize(avg = mean(estimate))  |> 
  mutate(category = "full")
```

```{r}
df2 = sim_result |> 
  filter(p.value < 0.05) |> 
  group_by(mu) |> 
  summarize(avg = mean(estimate)) |> 
  mutate(category="rejected")

df <- rbind(df1, df2)

ggplot(df, aes(x = mu, y = avg, color = category)) + 
  geom_point() +
  geom_line()
```

The sample average of mu hat across tests where the null is rejected approximates the true value of mu closely, especially as the true value of 
mu increases. 

## Problem 3

Import data

```{r}
data = read_csv("data/homicide-data.csv")
```

The are `r nrow(data)` rows and `r ncol(data)` in the raw data. Some variables include `city`, `state`, `victim_last`, `victim_first`, `victim_race`, `victim_age` and `victim_sex`.

```{r}
problem_3 = data |> 
  mutate(
    city_state = str_c(city, ", ", state),
    homicide = if_else(disposition %in% c("Closed without arrest", 
                                          "Open/No arrest"), "unsolved",
                       "solved")) |> 
  group_by(city_state, homicide) |> 
  summarize(n=n()) |> 
  pivot_wider(
    names_from = "homicide",
    values_from = "n"
  )
```

```{r}
problem_3 |> 
  filter(city_state == "Baltimore, MD") |> 
  mutate(tidy(prop.test(unsolved, solved+unsolved))) |> 
  select(estimate, conf.low, conf.high)
```

```{r}
result = problem_3 |> 
  drop_na() |> 
  mutate(
    test_result = map2(unsolved, solved + unsolved, ~ prop.test(.x, .y)),
    test_result_tidy = map(test_result, tidy)
  ) |> 
  unnest(test_result_tidy) |> 
  select(city_state, estimate, conf.low, conf.high)
```

```{r}
result |> 
  ggplot(aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  coord_flip() +
  labs(
    x = "City",
    y = "Proportion of Unsolved Homicides",
    title = "Proportion of Unsolved Homicides by City with Confidence Intervals"
  )  +
  theme_minimal()
```




